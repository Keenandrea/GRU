{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GRU.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Keenandrea/GRU/blob/master/GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOqC_JfZJu67",
        "colab_type": "text"
      },
      "source": [
        "# Our Setup\n",
        "---\n",
        "## import TensorFlow, import libraries\n",
        "---\n",
        "TensorFlow is an open-source software library employed for machine learning applications that we'll be leveraging to construct our neural network.\n",
        "\n",
        "Here the line **tf.enable_eager_execution()** allows us to apply eager execution to our model, an imperative programming environment that evaluates operations immediately. In short, it will equip us with a more interactive frontend to TensorFlow. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irZEv-0pJZpd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "from keras.utils import np_utils\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWRrFyjIKQ4b",
        "colab_type": "text"
      },
      "source": [
        "## download personal dataset\n",
        "---\n",
        "## mounting the *Google Drive*\n",
        "---\n",
        "To mount our *Google Drive* inside of a *Google Colab* instance, use the following commands:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSs8abgsKdlv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yyr9LypwOUxb",
        "colab_type": "text"
      },
      "source": [
        "All right. Now, with our *Google Drive* mounted, we can pull and push files between our *Drive* and our *Colab* instance with ease.\n",
        "\n",
        "First, let's download a textfile. To add flavor, we'll choose a textfile whose author not only writes masterfully, but stylistically, too. Check out [Gutenberg](https://www.gutenberg.org/catalog/). At the link, we'll find 59,000 books out of which we're invited to freely download as *.txt*.\n",
        "\n",
        "Once we've chosen our textfile, we'll open it in a notepad editor and delete the leading and trailing paragraphs that either introduce the text or explain Project Gutenberg's terms.\n",
        "\n",
        "With our textfile cleaned, we'll open our *Colab* and upload the texfile within. To do this, click the **>** slider menu near the upper-lefthand corner of your *Colab* notebook instance. At the top of the menu, click the leftmost tab labeled **Files** and click **UPLOAD**. Our *File Explorer* will open. From it, select the your textfile.\n",
        "\n",
        "---\n",
        "## read the data\n",
        "---\n",
        "Let's have a looksee at the text. We'll read it, decode it, and understand it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CDR-WYOXYVz",
        "colab_type": "code",
        "outputId": "d8386e4f-58ba-43ce-bac3-c4667bbcaacc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Read, then decode for py2 compatability\n",
        "text = open('metakaf.txt', 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 121108 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuMKPU5NajcZ",
        "colab_type": "text"
      },
      "source": [
        "Have a look at the first 50 characters in the text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3O1ZnnyKaOqS",
        "colab_type": "code",
        "outputId": "0a444bb4-5721-46ca-de4c-fc90c7d19545",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(text[:50])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "One morning, when Gregor Samsa woke from troubled \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFy-oWUBaooH",
        "colab_type": "text"
      },
      "source": [
        "Check out the unique characters in the file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eohkx48taung",
        "colab_type": "code",
        "outputId": "838c6ba1-8dca-494a-de81-7dd7659b2603",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "62 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8VSjydnazTl",
        "colab_type": "text"
      },
      "source": [
        "## process the text\n",
        "\n",
        "---\n",
        "Next things next, we need to map strings to numerical representation before training. How do we get this? We create two lookup tables:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZR2ak-wObBJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lookup table mapping from unique characters\n",
        "# to numbers\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "# lookup table mapping from numbers to characters\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sS46_D0FbdHq",
        "colab_type": "text"
      },
      "source": [
        "Run this cell to get the inside scoop the integer representation mapping for each character:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YhYTBUGbpj4",
        "colab_type": "code",
        "outputId": "e747026f-3541-4920-bef0-b8dc668f4e52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        }
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  '\\r':   1,\n",
            "  ' ' :   2,\n",
            "  '!' :   3,\n",
            "  '\"' :   4,\n",
            "  \"'\" :   5,\n",
            "  '(' :   6,\n",
            "  ')' :   7,\n",
            "  ',' :   8,\n",
            "  '-' :   9,\n",
            "  '.' :  10,\n",
            "  ':' :  11,\n",
            "  ';' :  12,\n",
            "  '?' :  13,\n",
            "  'A' :  14,\n",
            "  'B' :  15,\n",
            "  'C' :  16,\n",
            "  'D' :  17,\n",
            "  'E' :  18,\n",
            "  'F' :  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "so25COUkbzfL",
        "colab_type": "text"
      },
      "source": [
        "And so on. And this cell to show how the first 13 characters from the text are mapped to integers:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDIjXNMDbxLM",
        "colab_type": "code",
        "outputId": "6c2fdd2f-98e9-47d6-88c6-546f49d4074e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'One morning, ' ---- characters mapped to int ---- > [27 49 40  2 48 50 53 49 44 49 42  8  2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUjKz2wRb9-u",
        "colab_type": "text"
      },
      "source": [
        "## the prediction task\n",
        "\n",
        "---\n",
        "Let's say you're given a character, or even a sequence of characters. Based off that, if you're asked to predict the most probable next character, what sort of methodology would you employ?\n",
        "\n",
        "Answer or not, we're going to show you the *RNNs* methodology to this problem in the upcoming. First, know that RNNs maintain an internal state that depends on its previously seen elements to predict the next element.\n",
        "\n",
        "---\n",
        "\n",
        "## create training examples and targets\n",
        "\n",
        "---\n",
        "We divide the text into example sequences of specified length. Target sequences of the same length are corresponded to individual example sequences.\n",
        "\n",
        "For instance, say our specified length is 4 and our text is 'Bathe'. Under these conditions, the input sequence would be 'Bath', and the target sequence: 'athe'. And so on.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au713SVvertN",
        "colab_type": "code",
        "outputId": "49c08c51-a54c-4415-e979-1b68e4ae4096",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        }
      },
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//seq_length\n",
        "\n",
        "# create training examples and targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "O\n",
            "n\n",
            "e\n",
            " \n",
            "m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIqucDTte8E2",
        "colab_type": "text"
      },
      "source": [
        "Using the *batch* method, we easily convert these indicidual characters to sized sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-0-zTRafI-n",
        "colab_type": "code",
        "outputId": "00471c49-d9c9-4c96-ca4c-3dcc57c07a8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        }
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'One morning, when Gregor Samsa woke from troubled dreams, he found\\r\\nhimself transformed in his bed in'\n",
            "'to a horrible vermin.  He lay on\\r\\nhis armour-like back, and if he lifted his head a little he could\\r\\n'\n",
            "'see his brown belly, slightly domed and divided by arches into stiff\\r\\nsections.  The bedding was hard'\n",
            "'ly able to cover it and seemed ready\\r\\nto slide off any moment.  His many legs, pitifully thin compare'\n",
            "'d\\r\\nwith the size of the rest of him, waved about helplessly as he\\r\\nlooked.\\r\\n\\r\\n\"What\\'s happened to me?'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrmKly-qfO-Q",
        "colab_type": "text"
      },
      "source": [
        "By the *map* method, we apply a simple function to each batch, which duplicates and shifts to form the input and target for each sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqSTrmOmfeqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct1rEOLEfi9b",
        "colab_type": "text"
      },
      "source": [
        "For those visual learners out there:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b1YfyoJflSe",
        "colab_type": "code",
        "outputId": "db78706c-46e8-47c5-e5bd-5fafbe2dffa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'One morning, when Gregor Samsa woke from troubled dreams, he found\\r\\nhimself transformed in his bed i'\n",
            "Target data: 'ne morning, when Gregor Samsa woke from troubled dreams, he found\\r\\nhimself transformed in his bed in'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwvKQvyJfso7",
        "colab_type": "text"
      },
      "source": [
        "Each index of these vectors are processed as one single time step. Each timestep in course runs the same index prediction, however, the *RNN* also considers the previous step context as well as the current input character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcTYo-nfgJUC",
        "colab_type": "code",
        "outputId": "75314559-6c0d-4717-c17e-851e10332569",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 27 ('O')\n",
            "  expected output: 49 ('n')\n",
            "Step    1\n",
            "  input: 49 ('n')\n",
            "  expected output: 40 ('e')\n",
            "Step    2\n",
            "  input: 40 ('e')\n",
            "  expected output: 2 (' ')\n",
            "Step    3\n",
            "  input: 2 (' ')\n",
            "  expected output: 48 ('m')\n",
            "Step    4\n",
            "  input: 48 ('m')\n",
            "  expected output: 50 ('o')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa4id3gIgM4E",
        "colab_type": "text"
      },
      "source": [
        "## training batches\n",
        "\n",
        "---\n",
        "\n",
        "Before the model architecture feeds on our data, we shuffle the data and pack it into batches. Why are we shuffling the data? I'm glad you asked. Without getting too technical, shuffling is a solution to evaluation of the loss function **W** on the training dataset **X**. \n",
        "\n",
        "When **X** is unchanged over training iterations, the evaluation of **W** on **X** is a value regarded as the elevation of the surface. The surface will have numerous local minima. Gradient descent algorithms are susceptible to becoming stuck in these minima while better solutions may be nearby. \n",
        "\n",
        "By shuffling the rows, and then training on a subset, or, batch of **X** during every iteration, our **X** will change with the iteration. The result is the likely possibility that no two iterations over the entire sequence of training iterations and epochs will be performed on the exact same **X**.\n",
        "\n",
        "The affect allows us to easily leap from a local minimum and aquire a better **W**, which is a definite characteristic of a viable model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3D0Tgu28gbWo",
        "colab_type": "code",
        "outputId": "67234a6c-68b2-41fc-e013-356dcd41c29e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        }
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<DatasetV1Adapter shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9uVlVTKgzox",
        "colab_type": "text"
      },
      "source": [
        "## build that model architecture\n",
        "\n",
        "---\n",
        "\n",
        "Enough of that. Now let's get into the architecture of our network. Although you're welcome to add complexity, for this example, three layers define the architecture:\n",
        "\n",
        "1.   **Embedding**: this is the input layer. Think of it as a trainable lookup table that will map the numbers of each character to a vector with *embedding_dim* dimensions. Dimensionality of your embeddings is the length of the word vectors.\n",
        "2.   **GRU**: this is a type of RNN with a size *units=rnn_units*. Once again, to build a more complex architecture, you can emply *LSTM* layers here in place of the *GRU*.\n",
        "3.   **Dense**: this is the output layer, with *vocab_size* outputs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnhymlADqSmE",
        "colab_type": "code",
        "outputId": "8f5c8112-076e-4919-b3bc-7fd9ed652092",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "# embedding dimension \n",
        "embedding_dim = 256\n",
        "# number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9fe7004fbb3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# embedding dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0membedding_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# number of RNN units\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrnn_units\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vocab' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjWnVZkvqfIs",
        "colab_type": "text"
      },
      "source": [
        "Next we define a function to build the architecture. We use *CuDNNGRU* since we are running our model on *GPU*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZdZlHVcqypa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if tf.test.is_gpu_available():\n",
        "  rnn = tf.keras.layers.CuDNNGRU\n",
        "else:\n",
        "  import functools\n",
        "  rnn = functools.partial(\n",
        "    tf.keras.layers.GRU, recurrent_activation='sigmoid')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbETpf4iq0g3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    rnn(rnn_units,\n",
        "        return_sequences=True, \n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        stateful=True),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WD6C-SBrq3Xk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab), \n",
        "  embedding_dim=embedding_dim, \n",
        "  rnn_units=rnn_units, \n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMf8i-f5rA5O",
        "colab_type": "text"
      },
      "source": [
        "So, how does it work? For each character the model looks up the embedding, runs the *GRU* one timestep with the embedding as input, and then applies the dense layer to generate logits that will predicting the log-liklihood of the next character.\n",
        "\n",
        "Let's look under the hood. First, let's check the shape of the output:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrGYXmEDrjFS",
        "colab_type": "code",
        "outputId": "5d9537be-22c4-4c4f-a343-c18c4bedcc3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1): \n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape)\n",
        "  # (batch_size, sequence_length, vocab_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 62)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0v0Uc_GrxhB",
        "colab_type": "text"
      },
      "source": [
        "Now let's get a summary of the model architecture:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMNMvzTSr1FV",
        "colab_type": "code",
        "outputId": "70ebf35b-164c-48a3-9ca3-dc7130423d18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           15872     \n",
            "_________________________________________________________________\n",
            "cu_dnngru (CuDNNGRU)         (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 62)            63550     \n",
            "=================================================================\n",
            "Total params: 4,017,726\n",
            "Trainable params: 4,017,726\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0ueSrZrr66f",
        "colab_type": "text"
      },
      "source": [
        "To get actual predictions from the model, we need to sample from the output distribution, to get actual character indices. Distribution is defined by the logits over the character vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8J85N6tSsJ_J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seaCDySJsOu5",
        "colab_type": "text"
      },
      "source": [
        "So we get, at each timestep, a prediction of the next character index:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHvTL8NPsSZ3",
        "colab_type": "code",
        "outputId": "f33765aa-c2f4-468e-bafd-90383f2bb40c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([29, 40, 18, 45, 36,  7, 47, 17, 54, 36, 47, 39, 22,  3, 57,  5, 31,\n",
              "       41, 32,  4, 28, 45,  9, 53, 31, 20,  9,  3, 25, 25, 11, 60, 42, 22,\n",
              "       30, 22,  5, 46,  0, 32, 59, 39, 41, 49, 14, 19, 56, 25, 38, 24, 49,\n",
              "        5, 56, 55, 41, 27, 20, 27, 21, 16, 37, 27, 52,  1, 44,  3, 20,  8,\n",
              "       33, 34, 61, 51, 43, 35,  1, 54, 42, 57, 35, 30,  1, 16, 34, 36, 26,\n",
              "       61, 49, 48, 59, 13, 25, 59, 45, 29, 40, 27, 51, 24, 23, 50])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7XwpqBQsXp9",
        "colab_type": "text"
      },
      "source": [
        "Decoding these will show us the text predicted by this untrained model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jF8QfZdEscIc",
        "colab_type": "code",
        "outputId": "f1555d6d-c6d3-4acb-ad7b-654c8a29efe5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        }
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " 'would probably be the only one who would\\r\\ndare enter a room dominated by Gregor crawling about the b'\n",
            "\n",
            "Next Char Predictions: \n",
            " 'QeEja)lDsaldI!v\\'TfU\"Pj-rTG-!MM:ygISI\\'k\\nUxdfnAFuMcLn\\'utfOGOHCbOq\\ri!G,VWzphY\\rsgvYS\\rCWaNznmx?MxjQeOpLJo'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzp1_iVgseMk",
        "colab_type": "text"
      },
      "source": [
        "## train the model\n",
        "\n",
        "---\n",
        "\n",
        "Our problem can be treated as a standard classification problem. WHy? Because, given the previous *RNN* state, and the input this current time step, predict the **class** of the next character.\n",
        "\n",
        "---\n",
        "\n",
        "## optimizer and loss function attatched\n",
        "\n",
        "---\n",
        "\n",
        "Standard *sparse_softmax_crossentropy* loss function works in this case because it applies itself across the last dimension of the predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgLRiN8-tNVc",
        "colab_type": "code",
        "outputId": "fb9be5f8-2bf3-4fdf-d282-777c982199df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\") \n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 62)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.1263423\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCutSBUStShK",
        "colab_type": "text"
      },
      "source": [
        "Using *Adam* as our optimizer, we configure the training procedure and compile:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc5Eib9rtbIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "    optimizer = tf.train.AdamOptimizer(),\n",
        "    loss = loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkO2xVP1tdka",
        "colab_type": "text"
      },
      "source": [
        "## checkpoints\n",
        "\n",
        "---\n",
        "\n",
        "We will use checkpoints to ensure that checkpoints are saved during training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4cah4eatl00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JysAiBlTtoxf",
        "colab_type": "text"
      },
      "source": [
        "## execute the sucker\n",
        "\n",
        "---\n",
        "\n",
        "Define the number of epochs. Train network with *model.fit*. Save as *history* to compare metrics:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq02oFPLt7FB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS=100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ss-aBns1t-fR",
        "colab_type": "code",
        "outputId": "e7edeca1-e20a-430f-c1b7-dae3ffcb9ea2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3523
        }
      },
      "source": [
        "history = model.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "18/18 [==============================] - 2s 99ms/step - loss: 4.0771\n",
            "Epoch 2/100\n",
            "18/18 [==============================] - 1s 68ms/step - loss: 2.9561\n",
            "Epoch 3/100\n",
            "18/18 [==============================] - 1s 70ms/step - loss: 2.5801\n",
            "Epoch 4/100\n",
            "18/18 [==============================] - 1s 71ms/step - loss: 2.3585\n",
            "Epoch 5/100\n",
            "18/18 [==============================] - 1s 72ms/step - loss: 2.2304\n",
            "Epoch 6/100\n",
            "18/18 [==============================] - 1s 71ms/step - loss: 2.1554\n",
            "Epoch 7/100\n",
            "18/18 [==============================] - 1s 72ms/step - loss: 2.0911\n",
            "Epoch 8/100\n",
            "18/18 [==============================] - 1s 72ms/step - loss: 2.0299\n",
            "Epoch 9/100\n",
            "18/18 [==============================] - 1s 72ms/step - loss: 1.9723\n",
            "Epoch 10/100\n",
            "18/18 [==============================] - 1s 70ms/step - loss: 1.9114\n",
            "Epoch 11/100\n",
            "18/18 [==============================] - 1s 71ms/step - loss: 1.8482\n",
            "Epoch 12/100\n",
            "18/18 [==============================] - 1s 70ms/step - loss: 1.7893\n",
            "Epoch 13/100\n",
            "18/18 [==============================] - 1s 72ms/step - loss: 1.7315\n",
            "Epoch 14/100\n",
            "18/18 [==============================] - 1s 73ms/step - loss: 1.6759\n",
            "Epoch 15/100\n",
            "18/18 [==============================] - 1s 71ms/step - loss: 1.6228\n",
            "Epoch 16/100\n",
            "18/18 [==============================] - 1s 70ms/step - loss: 1.5706\n",
            "Epoch 17/100\n",
            "18/18 [==============================] - 1s 75ms/step - loss: 1.5235\n",
            "Epoch 18/100\n",
            "18/18 [==============================] - 1s 71ms/step - loss: 1.4730\n",
            "Epoch 19/100\n",
            "18/18 [==============================] - 1s 71ms/step - loss: 1.4325\n",
            "Epoch 20/100\n",
            "18/18 [==============================] - 1s 78ms/step - loss: 1.3921\n",
            "Epoch 21/100\n",
            "18/18 [==============================] - 1s 72ms/step - loss: 1.3517\n",
            "Epoch 22/100\n",
            "18/18 [==============================] - 1s 72ms/step - loss: 1.3151\n",
            "Epoch 23/100\n",
            "18/18 [==============================] - 1s 72ms/step - loss: 1.2811\n",
            "Epoch 24/100\n",
            "18/18 [==============================] - 1s 72ms/step - loss: 1.2449\n",
            "Epoch 25/100\n",
            "18/18 [==============================] - 1s 73ms/step - loss: 1.2112\n",
            "Epoch 26/100\n",
            "18/18 [==============================] - 1s 72ms/step - loss: 1.1777\n",
            "Epoch 27/100\n",
            "18/18 [==============================] - 1s 72ms/step - loss: 1.1435\n",
            "Epoch 28/100\n",
            "18/18 [==============================] - 1s 71ms/step - loss: 1.1109\n",
            "Epoch 29/100\n",
            "18/18 [==============================] - 1s 74ms/step - loss: 1.0802\n",
            "Epoch 30/100\n",
            "18/18 [==============================] - 1s 71ms/step - loss: 1.0466\n",
            "Epoch 31/100\n",
            "18/18 [==============================] - 1s 73ms/step - loss: 1.0170\n",
            "Epoch 32/100\n",
            "18/18 [==============================] - 1s 72ms/step - loss: 0.9829\n",
            "Epoch 33/100\n",
            "18/18 [==============================] - 1s 73ms/step - loss: 0.9444\n",
            "Epoch 34/100\n",
            "18/18 [==============================] - 1s 71ms/step - loss: 0.9085\n",
            "Epoch 35/100\n",
            "18/18 [==============================] - 1s 71ms/step - loss: 0.8766\n",
            "Epoch 36/100\n",
            "18/18 [==============================] - 1s 72ms/step - loss: 0.8368\n",
            "Epoch 37/100\n",
            "18/18 [==============================] - 1s 72ms/step - loss: 0.8023\n",
            "Epoch 38/100\n",
            "18/18 [==============================] - 1s 71ms/step - loss: 0.7632\n",
            "Epoch 39/100\n",
            "18/18 [==============================] - 1s 72ms/step - loss: 0.7248\n",
            "Epoch 40/100\n",
            "18/18 [==============================] - 1s 70ms/step - loss: 0.6892\n",
            "Epoch 41/100\n",
            "18/18 [==============================] - 1s 70ms/step - loss: 0.6495\n",
            "Epoch 42/100\n",
            "18/18 [==============================] - 1s 73ms/step - loss: 0.6127\n",
            "Epoch 43/100\n",
            "18/18 [==============================] - 1s 71ms/step - loss: 0.5758\n",
            "Epoch 44/100\n",
            "18/18 [==============================] - 1s 71ms/step - loss: 0.5392\n",
            "Epoch 45/100\n",
            "18/18 [==============================] - 1s 71ms/step - loss: 0.5073\n",
            "Epoch 46/100\n",
            "18/18 [==============================] - 1s 72ms/step - loss: 0.4762\n",
            "Epoch 47/100\n",
            "18/18 [==============================] - 1s 69ms/step - loss: 0.4470\n",
            "Epoch 48/100\n",
            "18/18 [==============================] - 1s 70ms/step - loss: 0.4224\n",
            "Epoch 49/100\n",
            "18/18 [==============================] - 1s 70ms/step - loss: 0.3964\n",
            "Epoch 50/100\n",
            "18/18 [==============================] - 1s 71ms/step - loss: 0.3692\n",
            "Epoch 51/100\n",
            "18/18 [==============================] - 1s 71ms/step - loss: 0.3526\n",
            "Epoch 52/100\n",
            "18/18 [==============================] - 1s 70ms/step - loss: 0.3319\n",
            "Epoch 53/100\n",
            "18/18 [==============================] - 1s 70ms/step - loss: 0.3180\n",
            "Epoch 54/100\n",
            "18/18 [==============================] - 1s 72ms/step - loss: 0.3020\n",
            "Epoch 55/100\n",
            "18/18 [==============================] - 1s 69ms/step - loss: 0.2905\n",
            "Epoch 56/100\n",
            "18/18 [==============================] - 1s 70ms/step - loss: 0.2784\n",
            "Epoch 57/100\n",
            "18/18 [==============================] - 1s 70ms/step - loss: 0.2665\n",
            "Epoch 58/100\n",
            "18/18 [==============================] - 1s 72ms/step - loss: 0.2586\n",
            "Epoch 59/100\n",
            "18/18 [==============================] - 1s 72ms/step - loss: 0.2484\n",
            "Epoch 60/100\n",
            "18/18 [==============================] - 1s 70ms/step - loss: 0.2427\n",
            "Epoch 61/100\n",
            "18/18 [==============================] - 1s 70ms/step - loss: 0.2362\n",
            "Epoch 62/100\n",
            "18/18 [==============================] - 1s 72ms/step - loss: 0.2288\n",
            "Epoch 63/100\n",
            "18/18 [==============================] - 1s 70ms/step - loss: 0.2229\n",
            "Epoch 64/100\n",
            "18/18 [==============================] - 1s 70ms/step - loss: 0.2172\n",
            "Epoch 65/100\n",
            "18/18 [==============================] - 1s 71ms/step - loss: 0.2131\n",
            "Epoch 66/100\n",
            "18/18 [==============================] - 1s 70ms/step - loss: 0.2110\n",
            "Epoch 67/100\n",
            "18/18 [==============================] - 1s 70ms/step - loss: 0.2051\n",
            "Epoch 68/100\n",
            "18/18 [==============================] - 1s 75ms/step - loss: 0.2005\n",
            "Epoch 69/100\n",
            "18/18 [==============================] - 1s 70ms/step - loss: 0.1977\n",
            "Epoch 70/100\n",
            "18/18 [==============================] - 1s 71ms/step - loss: 0.1935\n",
            "Epoch 71/100\n",
            "18/18 [==============================] - 1s 71ms/step - loss: 0.1913\n",
            "Epoch 72/100\n",
            "18/18 [==============================] - 1s 71ms/step - loss: 0.1879\n",
            "Epoch 73/100\n",
            "18/18 [==============================] - 1s 71ms/step - loss: 0.1861\n",
            "Epoch 74/100\n",
            "18/18 [==============================] - 1s 71ms/step - loss: 0.1834\n",
            "Epoch 75/100\n",
            "18/18 [==============================] - 1s 71ms/step - loss: 0.1806\n",
            "Epoch 76/100\n",
            "18/18 [==============================] - 1s 72ms/step - loss: 0.1778\n",
            "Epoch 77/100\n",
            "18/18 [==============================] - 1s 71ms/step - loss: 0.1787\n",
            "Epoch 78/100\n",
            "18/18 [==============================] - 1s 71ms/step - loss: 0.1743\n",
            "Epoch 79/100\n",
            "18/18 [==============================] - 1s 73ms/step - loss: 0.1719\n",
            "Epoch 80/100\n",
            "18/18 [==============================] - 1s 70ms/step - loss: 0.1707\n",
            "Epoch 81/100\n",
            "18/18 [==============================] - 1s 70ms/step - loss: 0.1665\n",
            "Epoch 82/100\n",
            "18/18 [==============================] - 1s 76ms/step - loss: 0.1695\n",
            "Epoch 83/100\n",
            "18/18 [==============================] - 1s 71ms/step - loss: 0.1667\n",
            "Epoch 84/100\n",
            "18/18 [==============================] - 1s 73ms/step - loss: 0.1672\n",
            "Epoch 85/100\n",
            "18/18 [==============================] - 1s 73ms/step - loss: 0.1651\n",
            "Epoch 86/100\n",
            "18/18 [==============================] - 1s 72ms/step - loss: 0.1630\n",
            "Epoch 87/100\n",
            "18/18 [==============================] - 1s 71ms/step - loss: 0.1589\n",
            "Epoch 88/100\n",
            "18/18 [==============================] - 1s 72ms/step - loss: 0.1618\n",
            "Epoch 89/100\n",
            "18/18 [==============================] - 1s 74ms/step - loss: 0.1573\n",
            "Epoch 90/100\n",
            "18/18 [==============================] - 1s 73ms/step - loss: 0.1552\n",
            "Epoch 91/100\n",
            "18/18 [==============================] - 1s 71ms/step - loss: 0.1570\n",
            "Epoch 92/100\n",
            "18/18 [==============================] - 1s 72ms/step - loss: 0.1558\n",
            "Epoch 93/100\n",
            "18/18 [==============================] - 1s 76ms/step - loss: 0.1521\n",
            "Epoch 94/100\n",
            "18/18 [==============================] - 1s 71ms/step - loss: 0.1508\n",
            "Epoch 95/100\n",
            "18/18 [==============================] - 1s 76ms/step - loss: 0.1525\n",
            "Epoch 96/100\n",
            "18/18 [==============================] - 1s 71ms/step - loss: 0.1501\n",
            "Epoch 97/100\n",
            "18/18 [==============================] - 1s 71ms/step - loss: 0.1515\n",
            "Epoch 98/100\n",
            "18/18 [==============================] - 1s 74ms/step - loss: 0.1485\n",
            "Epoch 99/100\n",
            "18/18 [==============================] - 1s 73ms/step - loss: 0.1491\n",
            "Epoch 100/100\n",
            "18/18 [==============================] - 1s 71ms/step - loss: 0.1474\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3PCuzw3uhp_",
        "colab_type": "text"
      },
      "source": [
        "## generate text\n",
        "\n",
        "---\n",
        "\n",
        "To restore checkpoints, you'll find a directory:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CveLmcwruuQu",
        "colab_type": "code",
        "outputId": "a1896435-c91f-438e-eb71-ab870a47f23b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_100'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDqb98zSuwY4",
        "colab_type": "text"
      },
      "source": [
        "Once more, build the architecture, this time using the weights from the checkpoint directory:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V81WqOeIuwI1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcrv0RvavDB5",
        "colab_type": "text"
      },
      "source": [
        "## loop of generation\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpgNRJj4vHYj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 2000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing) \n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 0.33\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a multinomial distribution to predict the word returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
        "      \n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "      \n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldCHq8K2vOqX",
        "colab_type": "text"
      },
      "source": [
        "Prompt generation with a *start_string*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kD7J1vnhvTaw",
        "colab_type": "code",
        "outputId": "b49e56e0-9f71-4bff-9b62-85dcaec3ebb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        }
      },
      "source": [
        "print(generate_text(model, start_string=u\"Time \"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time on her face as if she had some tremendous\r\n",
            "good news to report, but would only do it if she was clever; she was already\r\n",
            "intend, the other food there staring\r\n",
            "dry-eyed at the table.\r\n",
            "\r\n",
            "Gregor hardly slept at all, either night or day.  Sometimes tice than a month, and his condition seemed serious\r\n",
            "enough to remind even his father that Gregor, despite its breadth and its weight, the bulk of\r\n",
            "his body eventually followed slowly in the dirtter wanted to spare them what distress she could as they were\r\n",
            "indeed suffering enough.\r\n",
            "\r\n",
            "It was impossible for her to bring her mother out of her face as if she had some tremendous\r\n",
            "good news to report, but would only do it if she was clever; she was already\r\n",
            "in hear not even touched at all as if it could not be used any more.\r\n",
            "She quickly dropped it all into a bin, and a chair to the window, climbing up onto\r\n",
            "the sill and, propped up in the chair where the gentlemen and sat - leaving\r\n",
            "the chair where the gentlemen and sat - leaving\r\n",
            "the chair where the gentlemen and sat - leaving\r\n",
            "the chair where the gentlemen and sat - leaving\r\n",
            "the chair where the gentlemen and sat - leaving\r\n",
            "the chair where the gentlemen and sat - leaving\r\n",
            "the chair where the gentlemen and sat - leaving\r\n",
            "the chair where the gentlemen and sat - leaving\r\n",
            "the chair where the gentlemen and sat - leaving\r\n",
            "the chair where the gentlemen and sat - leaving\r\n",
            "the chair where the gentlemen and sat - leaving\r\n",
            "the chair where the gentlemen and sat - leaving\r\n",
            "the chair where the gentlemen and sat - leaving\r\n",
            "the chair where the gentlemen and sat - leaving\r\n",
            "the chair where the gentlemen and sat - leaving\r\n",
            "the chair where the gentlemen and sat - leaving\r\n",
            "the chair where the gentlemen and sat - leaving\r\n",
            "the chair where the gentlemen and sat - leaving\r\n",
            "the chair where the gentlemen and sat - leaving\r\n",
            "the chair where the gentlemen and sat - leaving\r\n",
            "the chair where the gentlemen and sat - leaving\r\n",
            "the chair where the gentlemen and sat - leaving\r\n",
            "the chair where the gentlemen and sat \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIfyfwq2veqV",
        "colab_type": "text"
      },
      "source": [
        "## customized training \n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70qWwPkPviWc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab), \n",
        "  embedding_dim=embedding_dim, \n",
        "  rnn_units=rnn_units, \n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssaNXbp1vjSe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.train.AdamOptimizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTyRqiNOvkqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 1\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    # initializing the hidden state at the start of every epoch\n",
        "    # initally hidden is None\n",
        "    hidden = model.reset_states()\n",
        "    \n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "          with tf.GradientTape() as tape:\n",
        "              # feeding the hidden state back into the model\n",
        "              # This is the interesting step\n",
        "              predictions = model(inp)\n",
        "              loss = tf.losses.sparse_softmax_cross_entropy(target, predictions)\n",
        "              \n",
        "          grads = tape.gradient(loss, model.trainable_variables)\n",
        "          optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "          if batch_n % 100 == 0:\n",
        "              template = 'Epoch {} Batch {} Loss {:.4f}'\n",
        "              print(template.format(epoch+1, batch_n, loss))\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "      model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
        "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bu0DLMgrCFLo",
        "colab_type": "text"
      },
      "source": [
        "# the end."
      ]
    }
  ]
}